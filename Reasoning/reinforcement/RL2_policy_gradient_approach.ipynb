{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reasoning {RL} -  2. Policy Gradient Approach.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajr141/experiments/blob/master/Reasoning/Reinforcement/Reasoning%20%7BRL%7D%20-%202%3A%20Policy_Gradient_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKFSfxpLKwvM",
        "colab_type": "text"
      },
      "source": [
        "In policy gradient Approach we learn what is the optimal action in a given state, where as in value base we learn how good a state is. In other words in value based each state has a value, where as in policy gradient based states themself dont have any value but we need obtained these value by compute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Pufx-kMs5H",
        "colab_type": "text"
      },
      "source": [
        "Source: https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1I7jyz-Moo5",
        "colab_type": "text"
      },
      "source": [
        "## Example 1 - MultiArm Bandit Problem\n",
        "\n",
        "Essentially, there are n-many slot machines, each with a different fixed payout probability. The goal is to discover the machine with the best payout, and maximize the returned reward by always choosing it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KIc-wZwKrmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpEVmoJ_K-_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#List out our bandits. Currently bandit 4 (index#3) is set to most often provide a positive reward.\n",
        "bandits = [0.2, 0, -0.2, -5]\n",
        "\n",
        "num_bandits = len(bandits)\n",
        "\n",
        "def pullBandit(bandit):\n",
        "\n",
        "    #Get a random number.\n",
        "    result = np.random.randn(1)\n",
        "    # As we can see bandit value for index 3 is -5 hence it has largest change of being greater then \n",
        "    if result > bandit:\n",
        "        #return a positive reward.\n",
        "        return 1\n",
        "    else:\n",
        "        #return a negative reward.\n",
        "        return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82etWhZ5NaYm",
        "colab_type": "text"
      },
      "source": [
        "The code below established our simple neural agent. It consists of a set of values for each of the bandits. Each value is an estimate of the value of the return from choosing the bandit. We use a policy gradient method to update the agent by moving the value for the selected action toward the recieved reward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyVse-xdNYND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "#These two lines established the feed-forward part of the network. This does the actual choosing.\n",
        "weights = tf.Variable(tf.ones([num_bandits]))\n",
        "chosen_action = tf.argmax(weights, 0)\n",
        "\n",
        "#The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
        "#to compute the loss, and use it to update the network.\n",
        "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
        "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
        "responsible_weight = tf.slice(weights, action_holder,[1])\n",
        "\n",
        "loss = -(tf.log(responsible_weight) * reward_holder)\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "update = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg-gux0gNuij",
        "colab_type": "code",
        "outputId": "29cfbde9-db5d-4e49-ff2c-52eb9d01f18c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "total_episodes = 1000   #Set total number of episodes to train agent on.\n",
        "total_reward = np.zeros(num_bandits) #Set scoreboard for bandits to 0.\n",
        "e = 0.1  #Set the chance of taking a random action.\n",
        "\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# Launch the tensorflow graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    i = 0\n",
        "\n",
        "    while i < total_episodes:\n",
        "        \n",
        "        #Choose either a random action or one from our network.\n",
        "        if np.random.rand(1) < e:\n",
        "            action = np.random.randint(num_bandits)\n",
        "        else:\n",
        "            action = sess.run(chosen_action)\n",
        "        \n",
        "        reward = pullBandit(bandits[action]) #Get our reward from picking one of the bandits.\n",
        "        \n",
        "        #Update the network.\n",
        "        _, resp, ww = sess.run([update, responsible_weight, weights], feed_dict={reward_holder:[reward], action_holder:[action]})\n",
        "        \n",
        "        #Update our running tally of scores.\n",
        "        total_reward[action] += reward\n",
        "        if i % 50 == 0:\n",
        "            print(\"Running reward for the \" + str(num_bandits) + \" bandits: \" + str(total_reward))\n",
        "        i+=1\n",
        "\n",
        "print(\"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
        "if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
        "    print(\"...and it was right!\")\n",
        "else:\n",
        "    print(\"...and it was wrong!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "Running reward for the 4 bandits: [1. 0. 0. 0.]\n",
            "Running reward for the 4 bandits: [ 0.  1.  0. 28.]\n",
            "Running reward for the 4 bandits: [-2.  1. -1. 75.]\n",
            "Running reward for the 4 bandits: [ -2.   0.  -1. 124.]\n",
            "Running reward for the 4 bandits: [ -2.   1.  -2. 170.]\n",
            "Running reward for the 4 bandits: [ -3.   1.  -2. 219.]\n",
            "Running reward for the 4 bandits: [ -5.  -2.  -1. 263.]\n",
            "Running reward for the 4 bandits: [ -5.  -2.   0. 308.]\n",
            "Running reward for the 4 bandits: [ -5.  -2.   2. 356.]\n",
            "Running reward for the 4 bandits: [ -4.   0.   2. 399.]\n",
            "Running reward for the 4 bandits: [ -3.  -4.   2. 442.]\n",
            "Running reward for the 4 bandits: [ -6.  -4.   2. 487.]\n",
            "Running reward for the 4 bandits: [ -5.  -4.   1. 533.]\n",
            "Running reward for the 4 bandits: [ -5.  -5.   0. 579.]\n",
            "Running reward for the 4 bandits: [ -6.  -5.   1. 627.]\n",
            "Running reward for the 4 bandits: [ -6.  -4.   0. 673.]\n",
            "Running reward for the 4 bandits: [ -3.  -4.   0. 720.]\n",
            "Running reward for the 4 bandits: [ -3.  -4.  -1. 769.]\n",
            "Running reward for the 4 bandits: [ -4.  -5.  -2. 816.]\n",
            "Running reward for the 4 bandits: [ -4.  -6.   1. 862.]\n",
            "The agent thinks bandit 4 is the most promising....\n",
            "...and it was right!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg4k6YAIgqgA",
        "colab_type": "text"
      },
      "source": [
        "## Example 2 - CartPole\n",
        "\n",
        "Above is a simple example, lets look at a more advance example which will interact with our env and will give the results which we can percieve. The Objective is to make cart pole stand as long as it can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKROUDkyhxU8",
        "colab_type": "text"
      },
      "source": [
        "By Default we cannot render Gym in Jupyter, since its a sandbox model and jupyter as of now doesnot support webGL, so we wil "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HNc0UmIioEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay ffmpeg > /dev/null 2>&1\n",
        "!apt-get install -y x11-utils > /dev/null 2>&1\n",
        "!pip install piglet pyglet\n",
        "!pip install gym==0.14.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdQFKdt3hxke",
        "colab_type": "code",
        "outputId": "1d26f9f0-073d-4898-d141-52faeba4e8a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1097'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1097'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNoRJVQ3t9bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' Some utility functions '''\n",
        "\n",
        "def show_video():  \n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiEROENoh4mM",
        "colab_type": "code",
        "outputId": "79a7c41d-b3e1-45dc-ef5c-4b37274cf07b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Wiki https://github.com/openai/gym/wiki/CartPole-v0\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "print(\"Observations:\", env.observation_space)  ## [position of cart, velocity of cart, angle of pole, rotation rate of pole]\n",
        "print(\"Actions:\", env.action_space)  ## Actions are LEFT and RIGHT"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observations: Box(4,)\n",
            "Actions: Discrete(2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnqWuqmtxKfu",
        "colab_type": "text"
      },
      "source": [
        "#### Lets try Random Actions Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ecba5cBAvNpu",
        "colab_type": "code",
        "outputId": "a6eb9e23-8158-4141-c6e4-be2b0a4f2e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "# Try running environment with random actions\n",
        "import time\n",
        "\n",
        "env = wrap_env(gym.make(\"CartPole-v0\"))\n",
        "env.reset()\n",
        "reward_sum = 0\n",
        "num_games = 10\n",
        "num_game = 0\n",
        "\n",
        "while num_game < num_games:\n",
        "    env.render()\n",
        "    observation, reward, done, _ = env.step(env.action_space.sample())\n",
        "    reward_sum += reward\n",
        "    if done:\n",
        "        print(\"Reward for this episode was: {}\".format(reward_sum))\n",
        "        reward_sum = 0\n",
        "        num_game += 1\n",
        "        env.reset()\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward for this episode was: 15.0\n",
            "Reward for this episode was: 24.0\n",
            "Reward for this episode was: 17.0\n",
            "Reward for this episode was: 23.0\n",
            "Reward for this episode was: 15.0\n",
            "Reward for this episode was: 12.0\n",
            "Reward for this episode was: 11.0\n",
            "Reward for this episode was: 17.0\n",
            "Reward for this episode was: 58.0\n",
            "Reward for this episode was: 18.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p6SMPJJxYAO",
        "colab_type": "text"
      },
      "source": [
        "<b>The only action we can take in this ENV is LEFT and RIGHT<b>\n",
        "  \n",
        "  #### Let try a Neural Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkmI4ZCJxltB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants defining our neural network\n",
        "hidden_layer_neurons = 10\n",
        "batch_size = 50\n",
        "learning_rate = 1e-2\n",
        "gamma = .99\n",
        "dimen = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go28GzdSxm-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "dbaf143e-cc53-4ad0-8c44-dc1a1fae0702"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Defining Graphs\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Define input placeholder\n",
        "observations = tf.placeholder(tf.float32, [None, dimen], name=\"input_x\")\n",
        "\n",
        "# First layer of weights\n",
        "W1 = tf.get_variable(\"W1\", shape=[dimen, hidden_layer_neurons],\n",
        "                    initializer=tf.contrib.layers.xavier_initializer())\n",
        "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
        "\n",
        "# Second layer of weights\n",
        "W2 = tf.get_variable(\"W2\", shape=[hidden_layer_neurons, 1],\n",
        "                    initializer=tf.contrib.layers.xavier_initializer())\n",
        "output = tf.nn.sigmoid(tf.matmul(layer1, W2))\n",
        "\n",
        "# We need to define the parts of the network needed for learning a policy\n",
        "trainable_vars = [W1, W2]\n",
        "\n",
        "input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
        "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
        "\n",
        "# Loss function\n",
        "log_lik = tf.log(input_y * (input_y - output) + \n",
        "                  (1 - input_y) * (input_y + output))\n",
        "loss = -tf.reduce_mean(log_lik * advantages)\n",
        "\n",
        "# Gradients\n",
        "new_grads = tf.gradients(loss, trainable_vars)\n",
        "W1_grad = tf.placeholder(tf.float32, name=\"batch_grad1\")\n",
        "W2_grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
        "\n",
        "# Learning\n",
        "batch_grad = [W1_grad, W2_grad]\n",
        "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "update_grads = adam.apply_gradients(zip(batch_grad, [W1, W2]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK2e-qJlx-4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discount_rewards(r, gamma=0.99):\n",
        "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
        "    e.g. f([1, 1, 1], 0.99) -> [1, 0.99, 0.9801]\n",
        "    \"\"\"\n",
        "    return np.array([val * (gamma ** i) for i, val in enumerate(r)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a57RixDjuypN",
        "colab_type": "code",
        "outputId": "ae365b0a-37b3-49a5-d775-0b2dd51d9c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "reward_sum = 0\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Placeholders for our observations, outputs and rewards\n",
        "xs = np.empty(0).reshape(0,dimen)\n",
        "ys = np.empty(0).reshape(0,1)\n",
        "rewards = np.empty(0).reshape(0,1)\n",
        "\n",
        "# Setting up our environment\n",
        "sess = tf.Session()\n",
        "rendering = False\n",
        "sess.run(init)\n",
        "observation = env.reset()\n",
        "\n",
        "# Placeholder for out gradients\n",
        "gradients = np.array([np.zeros(var.get_shape()) for var in trainable_vars])\n",
        "num_episodes = 50000\n",
        "num_episode = 0\n",
        "\n",
        "while num_episode < num_episodes:\n",
        "    #env.render()\n",
        "    # Append the observations to our batch\n",
        "    x = np.reshape(observation, [1, dimen])\n",
        "    \n",
        "    # Run the neural net to determine output\n",
        "    tf_prob = sess.run(output, feed_dict={observations: x})\n",
        "    \n",
        "    # Determine the output based on our net, allowing for some randomness\n",
        "    y = 0 if tf_prob > np.random.uniform() else 1\n",
        "    \n",
        "    # Append the observations and outputs for learning\n",
        "    xs = np.vstack([xs, x])\n",
        "    ys = np.vstack([ys, y])\n",
        "\n",
        "    # Determine the oucome of our action\n",
        "    observation, reward, done, _ = env.step(y)\n",
        "    reward_sum += reward\n",
        "    rewards = np.vstack([rewards, reward])\n",
        "    \n",
        "    ''' When Episode is finished, time to calculate discounted rewards and gradient'''\n",
        "    if done:\n",
        "        # Determine standardized rewards\n",
        "        discounted_rewards = discount_rewards(rewards, gamma)\n",
        "        discounted_rewards -= discounted_rewards.mean()\n",
        "        discounted_rewards /= discounted_rewards.std()\n",
        "\n",
        "        # Append gradients for case to running gradients\n",
        "        gradients += np.array(sess.run(new_grads, feed_dict={observations: xs,\n",
        "                                               input_y: ys,\n",
        "                                               advantages: discounted_rewards}))\n",
        "        \n",
        "        # Clear out game variables\n",
        "        xs = np.empty(0).reshape(0,dimen)\n",
        "        ys = np.empty(0).reshape(0,1)\n",
        "        rewards = np.empty(0).reshape(0,1)\n",
        "\n",
        "        # Once batch full\n",
        "        if num_episode % batch_size == 0:\n",
        "            # Updated gradients\n",
        "            sess.run(update_grads, feed_dict={W1_grad: gradients[0],\n",
        "                                             W2_grad: gradients[1]})\n",
        "            # Clear out gradients\n",
        "            gradients *= 0\n",
        "            \n",
        "            # Print status\n",
        "            print(\"Average reward for episode {}: {}\".format(num_episode, reward_sum/batch_size))\n",
        "            \n",
        "            ''' Break when reward is 200 '''\n",
        "            if reward_sum / batch_size >= 200:\n",
        "                print(\"Solved in {} episodes!\".format(num_episode))\n",
        "                break\n",
        "            reward_sum = 0\n",
        "        num_episode += 1\n",
        "        observation = env.reset()\n",
        "env.close()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average reward for episode 0: 0.6\n",
            "Average reward for episode 50: 21.82\n",
            "Average reward for episode 100: 22.48\n",
            "Average reward for episode 150: 25.64\n",
            "Average reward for episode 200: 23.24\n",
            "Average reward for episode 250: 23.5\n",
            "Average reward for episode 300: 25.38\n",
            "Average reward for episode 350: 24.1\n",
            "Average reward for episode 400: 24.76\n",
            "Average reward for episode 450: 26.92\n",
            "Average reward for episode 500: 26.5\n",
            "Average reward for episode 550: 29.16\n",
            "Average reward for episode 600: 30.06\n",
            "Average reward for episode 650: 26.64\n",
            "Average reward for episode 700: 34.82\n",
            "Average reward for episode 750: 34.06\n",
            "Average reward for episode 800: 35.86\n",
            "Average reward for episode 850: 30.38\n",
            "Average reward for episode 900: 33.7\n",
            "Average reward for episode 950: 35.16\n",
            "Average reward for episode 1000: 33.56\n",
            "Average reward for episode 1050: 31.62\n",
            "Average reward for episode 1100: 41.4\n",
            "Average reward for episode 1150: 34.5\n",
            "Average reward for episode 1200: 35.04\n",
            "Average reward for episode 1250: 43.74\n",
            "Average reward for episode 1300: 38.14\n",
            "Average reward for episode 1350: 39.04\n",
            "Average reward for episode 1400: 41.5\n",
            "Average reward for episode 1450: 44.44\n",
            "Average reward for episode 1500: 45.76\n",
            "Average reward for episode 1550: 40.36\n",
            "Average reward for episode 1600: 45.04\n",
            "Average reward for episode 1650: 48.88\n",
            "Average reward for episode 1700: 44.14\n",
            "Average reward for episode 1750: 48.44\n",
            "Average reward for episode 1800: 48.06\n",
            "Average reward for episode 1850: 43.8\n",
            "Average reward for episode 1900: 44.5\n",
            "Average reward for episode 1950: 54.84\n",
            "Average reward for episode 2000: 55.94\n",
            "Average reward for episode 2050: 56.94\n",
            "Average reward for episode 2100: 53.04\n",
            "Average reward for episode 2150: 60.76\n",
            "Average reward for episode 2200: 57.62\n",
            "Average reward for episode 2250: 52.6\n",
            "Average reward for episode 2300: 52.4\n",
            "Average reward for episode 2350: 56.7\n",
            "Average reward for episode 2400: 54.8\n",
            "Average reward for episode 2450: 56.92\n",
            "Average reward for episode 2500: 51.96\n",
            "Average reward for episode 2550: 53.36\n",
            "Average reward for episode 2600: 53.56\n",
            "Average reward for episode 2650: 58.64\n",
            "Average reward for episode 2700: 59.72\n",
            "Average reward for episode 2750: 67.28\n",
            "Average reward for episode 2800: 65.08\n",
            "Average reward for episode 2850: 68.14\n",
            "Average reward for episode 2900: 60.42\n",
            "Average reward for episode 2950: 65.42\n",
            "Average reward for episode 3000: 74.4\n",
            "Average reward for episode 3050: 68.6\n",
            "Average reward for episode 3100: 67.58\n",
            "Average reward for episode 3150: 62.66\n",
            "Average reward for episode 3200: 69.44\n",
            "Average reward for episode 3250: 66.7\n",
            "Average reward for episode 3300: 70.82\n",
            "Average reward for episode 3350: 73.84\n",
            "Average reward for episode 3400: 79.98\n",
            "Average reward for episode 3450: 87.56\n",
            "Average reward for episode 3500: 87.1\n",
            "Average reward for episode 3550: 93.6\n",
            "Average reward for episode 3600: 77.8\n",
            "Average reward for episode 3650: 96.24\n",
            "Average reward for episode 3700: 106.48\n",
            "Average reward for episode 3750: 109.34\n",
            "Average reward for episode 3800: 93.12\n",
            "Average reward for episode 3850: 112.52\n",
            "Average reward for episode 3900: 121.78\n",
            "Average reward for episode 3950: 115.4\n",
            "Average reward for episode 4000: 118.6\n",
            "Average reward for episode 4050: 124.78\n",
            "Average reward for episode 4100: 102.98\n",
            "Average reward for episode 4150: 125.38\n",
            "Average reward for episode 4200: 131.84\n",
            "Average reward for episode 4250: 118.52\n",
            "Average reward for episode 4300: 124.38\n",
            "Average reward for episode 4350: 135.44\n",
            "Average reward for episode 4400: 134.32\n",
            "Average reward for episode 4450: 132.02\n",
            "Average reward for episode 4500: 135.82\n",
            "Average reward for episode 4550: 146.94\n",
            "Average reward for episode 4600: 142.54\n",
            "Average reward for episode 4650: 146.48\n",
            "Average reward for episode 4700: 157.48\n",
            "Average reward for episode 4750: 157.68\n",
            "Average reward for episode 4800: 151.14\n",
            "Average reward for episode 4850: 176.88\n",
            "Average reward for episode 4900: 165.2\n",
            "Average reward for episode 4950: 183.44\n",
            "Average reward for episode 5000: 172.64\n",
            "Average reward for episode 5050: 171.08\n",
            "Average reward for episode 5100: 181.3\n",
            "Average reward for episode 5150: 183.56\n",
            "Average reward for episode 5200: 181.32\n",
            "Average reward for episode 5250: 180.7\n",
            "Average reward for episode 5300: 189.62\n",
            "Average reward for episode 5350: 184.02\n",
            "Average reward for episode 5400: 168.02\n",
            "Average reward for episode 5450: 181.02\n",
            "Average reward for episode 5500: 176.98\n",
            "Average reward for episode 5550: 179.0\n",
            "Average reward for episode 5600: 185.36\n",
            "Average reward for episode 5650: 170.04\n",
            "Average reward for episode 5700: 186.26\n",
            "Average reward for episode 5750: 175.56\n",
            "Average reward for episode 5800: 178.48\n",
            "Average reward for episode 5850: 185.74\n",
            "Average reward for episode 5900: 182.5\n",
            "Average reward for episode 5950: 185.32\n",
            "Average reward for episode 6000: 183.88\n",
            "Average reward for episode 6050: 171.6\n",
            "Average reward for episode 6100: 184.24\n",
            "Average reward for episode 6150: 186.78\n",
            "Average reward for episode 6200: 191.88\n",
            "Average reward for episode 6250: 182.68\n",
            "Average reward for episode 6300: 180.68\n",
            "Average reward for episode 6350: 190.32\n",
            "Average reward for episode 6400: 178.74\n",
            "Average reward for episode 6450: 183.6\n",
            "Average reward for episode 6500: 192.26\n",
            "Average reward for episode 6550: 191.68\n",
            "Average reward for episode 6600: 190.24\n",
            "Average reward for episode 6650: 188.72\n",
            "Average reward for episode 6700: 191.22\n",
            "Average reward for episode 6750: 192.34\n",
            "Average reward for episode 6800: 193.46\n",
            "Average reward for episode 6850: 189.5\n",
            "Average reward for episode 6900: 192.48\n",
            "Average reward for episode 6950: 196.12\n",
            "Average reward for episode 7000: 198.18\n",
            "Average reward for episode 7050: 191.34\n",
            "Average reward for episode 7100: 196.08\n",
            "Average reward for episode 7150: 196.64\n",
            "Average reward for episode 7200: 197.28\n",
            "Average reward for episode 7250: 194.14\n",
            "Average reward for episode 7300: 192.72\n",
            "Average reward for episode 7350: 196.9\n",
            "Average reward for episode 7400: 196.06\n",
            "Average reward for episode 7450: 194.4\n",
            "Average reward for episode 7500: 193.7\n",
            "Average reward for episode 7550: 195.54\n",
            "Average reward for episode 7600: 198.86\n",
            "Average reward for episode 7650: 191.36\n",
            "Average reward for episode 7700: 196.16\n",
            "Average reward for episode 7750: 194.72\n",
            "Average reward for episode 7800: 194.82\n",
            "Average reward for episode 7850: 188.94\n",
            "Average reward for episode 7900: 195.28\n",
            "Average reward for episode 7950: 193.12\n",
            "Average reward for episode 8000: 197.48\n",
            "Average reward for episode 8050: 198.06\n",
            "Average reward for episode 8100: 191.16\n",
            "Average reward for episode 8150: 195.94\n",
            "Average reward for episode 8200: 195.96\n",
            "Average reward for episode 8250: 193.72\n",
            "Average reward for episode 8300: 192.58\n",
            "Average reward for episode 8350: 194.02\n",
            "Average reward for episode 8400: 194.56\n",
            "Average reward for episode 8450: 192.92\n",
            "Average reward for episode 8500: 194.44\n",
            "Average reward for episode 8550: 196.96\n",
            "Average reward for episode 8600: 192.62\n",
            "Average reward for episode 8650: 193.94\n",
            "Average reward for episode 8700: 195.56\n",
            "Average reward for episode 8750: 196.58\n",
            "Average reward for episode 8800: 197.36\n",
            "Average reward for episode 8850: 194.72\n",
            "Average reward for episode 8900: 195.1\n",
            "Average reward for episode 8950: 188.66\n",
            "Average reward for episode 9000: 193.8\n",
            "Average reward for episode 9050: 190.16\n",
            "Average reward for episode 9100: 197.76\n",
            "Average reward for episode 9150: 193.28\n",
            "Average reward for episode 9200: 195.38\n",
            "Average reward for episode 9250: 197.84\n",
            "Average reward for episode 9300: 199.2\n",
            "Average reward for episode 9350: 198.7\n",
            "Average reward for episode 9400: 193.52\n",
            "Average reward for episode 9450: 196.14\n",
            "Average reward for episode 9500: 197.08\n",
            "Average reward for episode 9550: 197.24\n",
            "Average reward for episode 9600: 199.78\n",
            "Average reward for episode 9650: 198.18\n",
            "Average reward for episode 9700: 197.22\n",
            "Average reward for episode 9750: 196.32\n",
            "Average reward for episode 9800: 196.54\n",
            "Average reward for episode 9850: 196.68\n",
            "Average reward for episode 9900: 196.54\n",
            "Average reward for episode 9950: 199.58\n",
            "Average reward for episode 10000: 198.66\n",
            "Average reward for episode 10050: 196.74\n",
            "Average reward for episode 10100: 197.72\n",
            "Average reward for episode 10150: 197.12\n",
            "Average reward for episode 10200: 193.92\n",
            "Average reward for episode 10250: 198.24\n",
            "Average reward for episode 10300: 198.68\n",
            "Average reward for episode 10350: 198.32\n",
            "Average reward for episode 10400: 198.98\n",
            "Average reward for episode 10450: 198.58\n",
            "Average reward for episode 10500: 197.62\n",
            "Average reward for episode 10550: 197.08\n",
            "Average reward for episode 10600: 192.66\n",
            "Average reward for episode 10650: 199.34\n",
            "Average reward for episode 10700: 197.64\n",
            "Average reward for episode 10750: 197.96\n",
            "Average reward for episode 10800: 199.58\n",
            "Average reward for episode 10850: 198.82\n",
            "Average reward for episode 10900: 197.26\n",
            "Average reward for episode 10950: 197.3\n",
            "Average reward for episode 11000: 199.78\n",
            "Average reward for episode 11050: 193.8\n",
            "Average reward for episode 11100: 196.24\n",
            "Average reward for episode 11150: 191.26\n",
            "Average reward for episode 11200: 196.1\n",
            "Average reward for episode 11250: 200.0\n",
            "Solved in 11250 episodes!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FU84iQ-1qAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm -rf video"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1JHvg8g1EsD",
        "colab_type": "text"
      },
      "source": [
        "**Lets See our trained bot in action**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_UxSVRv1AbW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "8d155e01-a23f-4982-edb6-3e8ea4e8b2a3"
      },
      "source": [
        "env = wrap_env(gym.make(\"CartPole-v0\"))\n",
        "observation = env.reset()\n",
        "observation\n",
        "reward_sum = 0\n",
        "\n",
        "while True:\n",
        "    env.render()\n",
        "    x = np.reshape(observation, [1, dimen])\n",
        "    y = sess.run(output, feed_dict={observations: x})\n",
        "    y = 0 if y > 0.5 else 1\n",
        "    observation, reward, done, _ = env.step(y)\n",
        "    reward_sum += reward\n",
        "    if done:\n",
        "        print(\"Total score: {}\".format(reward_sum))\n",
        "        break\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total score: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}