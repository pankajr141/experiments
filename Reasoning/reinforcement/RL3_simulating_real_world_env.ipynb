{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reasoning {RL} - 3: Simulating Real world Env.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajr141/experiments/blob/master/Reasoning/Reinforcement/Reasoning_%7BRL%7D_3_Simulating_Real_world_Env.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU1ivoHOgTDt",
        "colab_type": "text"
      },
      "source": [
        "Till now the env we are using are simulation and not the real words. **How do we even know how well our agent is going to work in real world ? We don't.**\n",
        "\n",
        "The success of agent will only depend on how close our simulated env is to real world. We can't train our agent in real world env as it is slow and our agent in fragile (eg car learning to drive, we can't let each car crash till our agent learn to drive).\n",
        "\n",
        "This section mostly focus on how do we simulate a env in which our agent can learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqMcoL84hUW_",
        "colab_type": "text"
      },
      "source": [
        "## Objective\n",
        "\n",
        "**Our objective is to train a model which works like real env, and then train our agent on that model.**\n",
        "\n",
        "Our training will switch b/w learning model using real env and training agent on model.\n",
        "\n",
        "*For the simplicty lets assume our real world is gym env and we will create our own env as model. Tricky right when the real words is just a env, but we cannot have real world example in this notebook as it will require actual sensors and other physical objects.* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZE0960WjFSu",
        "colab_type": "text"
      },
      "source": [
        "## Let's code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMkSC_oOf_jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7nI74_zi2I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"CartPole-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMLfTUMRi41U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 1e-2 # Learning rate, applicable to both nn, policy and model\n",
        "gamma = 0.99 # Discount factor for rewards\n",
        "decay_rate = 0.99 # Decay factor for RMSProp leaky sum of grad**2\n",
        "model_batch_size = 3 # Batch size used for training model nn\n",
        "policy_batch_size = 3 # Batch size used for training policy nn\n",
        "dimen = 4 # Number of dimensions in the environment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8JoX47WjCSC",
        "colab_type": "text"
      },
      "source": [
        "### Model N/W - Simulating our Real World Env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqdyO30sjRtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "num_hidden_m = 256\n",
        "\n",
        "# Dimensions of the previous state plus 1 for the action, standard || s', r, d = f(s, a)\n",
        "dimen_m = dimen + 1\n",
        "\n",
        "# Placeholder for inputs\n",
        "input_x_m = tf.placeholder(tf.float32, [None, dimen_m])\n",
        "\n",
        "# First layer\n",
        "W1_m = tf.get_variable(\"W1_m\", shape=[dimen_m, num_hidden_m], initializer=tf.contrib.layers.xavier_initializer())\n",
        "B1_m = tf.Variable(tf.zeros([num_hidden_m]), name=\"B1M\")\n",
        "layer1_m = tf.nn.relu(tf.matmul(input_x_m, W1_m) + B1_m)\n",
        "\n",
        "# Second layer\n",
        "W2_m = tf.get_variable(\"W2_m\", shape=[num_hidden_m, num_hidden_m], initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "B2_m = tf.Variable(tf.zeros([num_hidden_m]), name=\"B2_m\")\n",
        "layer2_m = tf.nn.relu(tf.matmul(layer1_m, W2_m) + B2_m)\n",
        "\n",
        "# Third (output) layers\n",
        "# Note that there are three separate output layers, \n",
        "# one for next observation, reward and whether the game is complete\n",
        "\n",
        "W_obs_m = tf.get_variable(\"W_obs_m\", shape=[num_hidden_m, 4],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "B_obs_m = tf.Variable(tf.zeros([4]), name=\"B_obs_m\")\n",
        "\n",
        "W_reward_m = tf.get_variable(\"W_reward_m\", shape=[num_hidden_m, 1],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "B_reward_m = tf.Variable(tf.zeros([1]), name=\"B_reward_m\")\n",
        "\n",
        "W_done_m = tf.get_variable(\"W_done_m\", shape=[num_hidden_m,1],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "B_done_m = tf.Variable(tf.zeros([1]), name=\"B_done_m\")\n",
        "\n",
        "output_obs_m = tf.matmul(layer2_m, W_obs_m) + B_obs_m\n",
        "output_reward_m = tf.matmul(layer2_m, W_reward_m) + B_reward_m\n",
        "output_done_m = tf.sigmoid(tf.matmul(layer2_m, W_done_m) + B_done_m)\n",
        "\n",
        "# Placeholders for inputs used in training\n",
        "actual_obs_m = tf.placeholder(tf.float32, [None, dimen_m], name=\"actual_obs\")\n",
        "actual_reward_m = tf.placeholder(tf.float32, [None, 1], name=\"actual_reward\")\n",
        "actual_done_m = tf.placeholder(tf.float32, [None, 1], name=\"actual_done\")\n",
        "\n",
        "# Putting it all together\n",
        "predicted_state_m = tf.concat([output_obs_m, output_reward_m, output_done_m], axis=1)\n",
        "\n",
        "# Loss functions\n",
        "loss_obs_m = tf.square(actual_reward_m - output_reward_m)\n",
        "loss_reward_m = tf.square(actual_reward_m - output_reward_m)\n",
        "loss_done_m = -tf.log(actual_done_m * output_done_m + (1 - actual_done_m) * (1 - output_done_m))\n",
        "\n",
        "# Model loss is simply the average loss of the three outputs\n",
        "loss_m = tf.reduce_max(loss_obs_m + loss_reward_m + loss_done_m)\n",
        "\n",
        "adam_m = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "update_m = adam_m.minimize(loss_m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O961Oo3dkIH6",
        "colab_type": "text"
      },
      "source": [
        "### Policy N/W - Agent learning a policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXjdrjO8kHGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_hidden_p = 10 # Number of hidden units in the nn used to determine policy\n",
        "\n",
        "input_x_p = tf.placeholder(tf.float32, [None, dimen], name=\"input_x\")\n",
        "\n",
        "# First layerhttp://localhost:8888/notebooks/Documents/notebooks/rl_3-%20Model-Based%20RL.ipynb#Policy-Neural-Network\n",
        "W1_p = tf.get_variable(\"W1\", shape=[dimen,num_hidden_p], initializer=tf.contrib.layers.xavier_initializer())\n",
        "layer1_p = tf.nn.relu(tf.matmul(input_x_p, W1_p))\n",
        "\n",
        "# Second layer\n",
        "W2_p = tf.get_variable(\"W2\", shape=[num_hidden_p, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
        "output_p = tf.nn.sigmoid(tf.matmul(layer1_p, W2_p))\n",
        "\n",
        "# Placeholders for inputs used in training\n",
        "input_y_p = tf.placeholder(tf.float32, shape=[None, 1], name=\"input_y\")\n",
        "advantages_p = tf.placeholder(tf.float32, shape=[None,1], name=\"reward_signal\")\n",
        "\n",
        "# Loss function\n",
        "# Below is equivalent to: 0 if input_y_p == output_p else 1\n",
        "log_lik_p = tf.log(input_y_p * (input_y_p - output_p) + \n",
        "                 (1 - input_y_p) * (input_y_p + output_p))\n",
        "\n",
        "# We'll be trying to maximize log liklihood\n",
        "loss_p = -tf.reduce_mean(log_lik_p * advantages_p)\n",
        "\n",
        "# Gradients\n",
        "W1_grad_p = tf.placeholder(tf.float32,name=\"W1_grad\")\n",
        "W2_grad_p = tf.placeholder(tf.float32,name=\"W2_grad\")\n",
        "batch_grad_p = [W1_grad_p, W2_grad_p]\n",
        "trainable_vars_p = [W1_p, W2_p]\n",
        "grads_p = tf.gradients(loss_p, trainable_vars_p)\n",
        "\n",
        "# Optimizer\n",
        "adam_p = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "# Update function\n",
        "update_grads_p = adam_p.apply_gradients(zip(batch_grad_p, [W1_p, W2_p]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT-nrDTYkn0I",
        "colab_type": "text"
      },
      "source": [
        "### Just Checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4lJlT8Nkfjl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ffe21503-d4c4-4c98-bdcb-4122aed38842"
      },
      "source": [
        "# Initialize and test to see models are setup correctly\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "random_obs = np.random.random(size=[1, env.observation_space.shape[0]])\n",
        "random_action = env.action_space.sample()\n",
        "\n",
        "print(\"obs: {}\\naction: {}\\noutput obs: {}\\nouput reward: {}\\noutput done: {}\\noutput policy: {}\".format(\n",
        "        random_obs,\n",
        "        random_action,\n",
        "        sess.run(output_obs_m,feed_dict={input_x_m: np.hstack([random_obs, [[random_action]]])}),\n",
        "        sess.run(output_reward_m,feed_dict={input_x_m: np.hstack([random_obs, [[random_action]]])}),\n",
        "        sess.run(output_done_m,feed_dict={input_x_m: np.hstack([random_obs, [[random_action]]])}),\n",
        "        sess.run(output_p,feed_dict={input_x_p: random_obs})))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "obs: [[0.24976274 0.05879473 0.73273779 0.88921673]]\n",
            "action: 1\n",
            "output obs: [[-0.00857412 -0.09878153 -0.12084708  0.2065549 ]]\n",
            "ouput reward: [[0.05972929]]\n",
            "output done: [[0.533011]]\n",
            "output policy: [[0.5242924]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIE5oFRCk2_F",
        "colab_type": "text"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Okv7dfk2w3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discount(r, gamma=0.99, standardize=False):\n",
        "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
        "    e.g. f([1, 1, 1], 0.99) -> [1, 0.99, 0.9801]\"\"\"\n",
        "    \n",
        "    discounted = np.array([val * (gamma ** i) for i, val in enumerate(r)])\n",
        "    if standardize:\n",
        "        discounted -= np.mean(discounted)\n",
        "        discounted /= np.std(discounted)\n",
        "    return discounted\n",
        "\n",
        "def step_model(sess, xs, action):\n",
        "    \"\"\" Uses our trained nn model to produce a new state given a previous state and action \"\"\"\n",
        "    \n",
        "    # Last state\n",
        "    x = xs[-1].reshape(1,-1)\n",
        "    \n",
        "    # Append action\n",
        "    x = np.hstack([x, [[action]]])\n",
        "    \n",
        "    # Predict output\n",
        "    output_y = sess.run(predicted_state_m, feed_dict={input_x_m: x})\n",
        "    \n",
        "    # predicted_state_m == [state_0, state_1, state_2, state_3, reward, done]\n",
        "    output_next_state = output_y[:,:4]\n",
        "    output_reward = output_y[:,4]\n",
        "    output_done = output_y[:,5]\n",
        "    \n",
        "    # First and third env outputs are limited to +/- 2.4 and +/- 0.4\n",
        "    output_next_state[:,0] = np.clip(output_next_state[:,0],-2.4, 2.4)\n",
        "    \n",
        "    output_next_state[:,2] = np.clip(output_next_state[:,2],-0.4, 0.4)\n",
        "    \n",
        "    # Threshold for being done is likliehood being > 0.01\n",
        "    output_done = True if output_done > 0.01 or len(xs) > 500 else False\n",
        "    \n",
        "    return output_next_state, output_reward, output_done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMvWgwZXljFZ",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "\n",
        "Training the model and policy will happen simultaneously. We will have to train the model first before we use it in training the policy, but afterward, the policy will switch off between training from the real environment and the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP4zOkOvk2ze",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c3c283dd-efe3-476b-8e15-041b07cf82bf"
      },
      "source": [
        "# Tracks the score on the real (non-simulated) environment to determine when to stop\n",
        "real_rewards = []\n",
        "num_episodes = 5000\n",
        "\n",
        "# Trigger used to decide whether we should train from model or from real environment\n",
        "train_from_model = False\n",
        "train_first_steps = 500\n",
        "\n",
        "# Setup array to keep track of observations, rewards and actions\n",
        "observations = np.empty(0).reshape(0,dimen)\n",
        "rewards = np.empty(0).reshape(0,1)\n",
        "actions = np.empty(0).reshape(0,1)\n",
        "\n",
        "# Gradients\n",
        "grads = np.array([np.zeros(var.get_shape().as_list()) for var in trainable_vars_p])\n",
        "\n",
        "num_episode = 0\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "while num_episode < num_episodes:\n",
        "    observation = observation.reshape(1,-1)\n",
        "    \n",
        "    # Determine the policy\n",
        "    policy = sess.run(output_p, feed_dict={input_x_p: observation})\n",
        "    \n",
        "    # Decide on an action based on the policy, allowing for some randomness\n",
        "    action = 0 if policy > np.random.uniform() else 1\n",
        "\n",
        "    # Keep track of the observations and actions\n",
        "    observations = np.vstack([observations, observation])\n",
        "    actions = np.vstack([actions, action])\n",
        "    \n",
        "    # Determine next observation either from model or real environment\n",
        "    if train_from_model:\n",
        "        observation, reward, done = step_model(sess, observations, action)\n",
        "    else:\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "        \n",
        "    # Keep track of rewards\n",
        "    rewards = np.vstack([rewards, reward])\n",
        "    dones = np.zeros(shape=(len(observations),1))\n",
        "    \n",
        "    # If game is over or running long\n",
        "    if done or len(observations) > 200:\n",
        "        print(\"\\r{} / {} Training From Model: {}\\t\".format(num_episode, num_episodes, train_from_model), end=\"\")\n",
        "\n",
        "        # If we're not training our policy from our model, we'll train our model from the real env\n",
        "        if not train_from_model:\n",
        "             # Previous state and actions for training model\n",
        "            states = np.hstack([observations, actions])\n",
        "            prev_states = states[:-1,:]\n",
        "            next_states = states[1:, :]\n",
        "            next_rewards = rewards[1:, :]\n",
        "            next_dones = dones[1:, :]\n",
        "\n",
        "            feed_dict = {input_x_m: prev_states.astype(np.float32), \n",
        "                         actual_obs_m: next_states.astype(np.float32),\n",
        "                        actual_done_m: next_dones.astype(np.float32),\n",
        "                        actual_reward_m: next_rewards.astype(np.float32)}\n",
        "\n",
        "            loss, _ = sess.run([loss_m, update_m], feed_dict=feed_dict)\n",
        "            \n",
        "            real_rewards.append(sum(rewards))\n",
        "            \n",
        "        \n",
        "        # Discount rewards\n",
        "        disc_rewards = discount(rewards, standardize=True)\n",
        "        \n",
        "        # Add gradients to running batch\n",
        "        grads += sess.run(grads_p, feed_dict={input_x_p: observations,\n",
        "                                            input_y_p: actions,\n",
        "                                            advantages_p: disc_rewards})\n",
        "        \n",
        "        num_episode += 1\n",
        "        \n",
        "        observation = env.reset()\n",
        "\n",
        "        # Reset everything\n",
        "        observations = np.empty(0).reshape(0,dimen)\n",
        "        rewards = np.empty(0).reshape(0,1)\n",
        "        actions = np.empty(0).reshape(0,1)\n",
        "        \n",
        "        # Toggle between training from model and from real environment allowing sufficient time \n",
        "        # to train the model before its used for learning policy, it will be executed at each episode\n",
        "        if num_episode > train_first_steps:\n",
        "            train_from_model = not train_from_model \n",
        "\n",
        "        # If batch full\n",
        "        if num_episode % policy_batch_size == 0:\n",
        "            \n",
        "            # Update gradients\n",
        "            sess.run(update_grads_p, feed_dict={W1_grad_p: grads[0], W2_grad_p: grads[1]})\n",
        "            \n",
        "            # Reset gradients\n",
        "            grads = np.array([np.zeros(var.get_shape().as_list()) for var in trainable_vars_p])\n",
        "            \n",
        "            # Print periodically\n",
        "            if (num_episode % (100 * policy_batch_size) == 0):\n",
        "                print(\"Episode {} last batch rewards: {}\".format(\n",
        "                        num_episode, sum(real_rewards[-policy_batch_size:])/policy_batch_size))\n",
        "            \n",
        "            # If our real score is good enough, quit\n",
        "            if (sum(real_rewards[-10:]) / 10. >= 200):\n",
        "                print(\"Episode {} Training complete with total score of: {}\".format(\n",
        "                        num_episode, sum(real_rewards[-policy_batch_size:])/policy_batch_size))\n",
        "                break"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "299 / 5000 Training From Model: False\tEpisode 300 last batch rewards: [66.]\n",
            "599 / 5000 Training From Model: True\tEpisode 600 last batch rewards: [139.33333333]\n",
            "899 / 5000 Training From Model: True\tEpisode 900 last batch rewards: [169.66666667]\n",
            "1199 / 5000 Training From Model: True\tEpisode 1200 last batch rewards: [179.33333333]\n",
            "1499 / 5000 Training From Model: True\tEpisode 1500 last batch rewards: [184.66666667]\n",
            "1760 / 5000 Training From Model: False\tEpisode 1761 Training complete with total score of: [200.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}