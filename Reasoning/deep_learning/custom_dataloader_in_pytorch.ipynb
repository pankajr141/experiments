{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCLehn6X0Ktln6UQBc4kzZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajr141/experiments/blob/master/Reasoning/deeplearning/custom_dataloader_in_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective\n",
        "The input data will be stored in multiple pickle files. \n",
        "\n",
        "<pre>\n",
        "x_0.pkl  - this file contains X input sample of 1st pickle\n",
        "y_0.pkl  - this file contains Y output sample of 1st pickle\n",
        "</pre>\n",
        "\n",
        "Each File will have  <br>\n",
        "<pre>\n",
        "N, T, F \n",
        "N - Number of sample\n",
        "T - number of timesteps per sample\n",
        "F - number of feature per sample\n",
        "</pre>\n",
        "\n",
        "<p>\n",
        "We will create 2 dataloader 1 for train and 1 for test such that Each dataloader will load data from X and Y pickle in a way, that we notice no difference if all the dataset is in memory.\n",
        "Meaning when pulling a batch of size 8, each sample can come from different pickle.\n",
        "\n",
        "This solution ensures that very large dataset can be loaded like normal dataset if we have enough diskspace, however time required for each passthrough will significantly increase depending on IO speed of underlying hardware.\n",
        "</p>\n",
        "\n",
        "Only the mapping index which map location of index to pickle file will be kept in memory"
      ],
      "metadata": {
        "id": "6qHdCgg_q8OF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "Lets create K number of pickle each containing N number of records"
      ],
      "metadata": {
        "id": "rxPbkdOusggO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = 53     # number of pickle files\n",
        "N = 2000   # number of records per pickle\n",
        "Tx = 11    # time dimension for X \n",
        "Ty = 3     # time dimension for Y\n",
        "F = 6      # number of features"
      ],
      "metadata": {
        "id": "0H17pfhwrc6L"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "# Create directory where pickle files will be stored\n",
        "dataset_dir = \"dataset\"\n",
        "shutil.rmtree(dataset_dir)\n",
        "os.makedirs(dataset_dir)\n",
        "\n",
        "# Loop over each pickle file\n",
        "for i in range(K):\n",
        "  data_x = np.random.randint(1, 100, size=(N, Tx, F))\n",
        "  data_y = np.random.randint(1, 100, size=(N, Ty, F))\n",
        "\n",
        "  # pickle file for X dataset\n",
        "  pickle_file = os.path.join(dataset_dir, f\"x_{i}.pkl\") \n",
        "  with open(pickle_file, 'wb') as f:\n",
        "    pickle.dump(data_x, f)\n",
        "\n",
        "  # pickle file for Y dataset\n",
        "  pickle_file = os.path.join(dataset_dir, f\"y_{i}.pkl\") \n",
        "  with open(pickle_file, 'wb') as f:\n",
        "    pickle.dump(data_y, f)"
      ],
      "metadata": {
        "id": "BHIUXsHAs6YJ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now since the dataset in saved in dataset_dir let check the files"
      ],
      "metadata": {
        "id": "ijcjRvrmuF-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "picklefiles = [os.path.join(dataset_dir, x) for x in os.listdir(dataset_dir)]x\n",
        "\n",
        "# Lets filter Y files from our pickle dataset, for now we only need X files\n",
        "picklefiles = list(filter(lambda x: \"x_\" in x, picklefiles))\n",
        "print(picklefiles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR11BIoduMa2",
        "outputId": "f993a7f1-efd6-4770-ac47-daf0a563fcc9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dataset/x_47.pkl', 'dataset/x_5.pkl', 'dataset/x_22.pkl', 'dataset/x_20.pkl', 'dataset/x_42.pkl', 'dataset/x_39.pkl', 'dataset/x_46.pkl', 'dataset/x_25.pkl', 'dataset/x_34.pkl', 'dataset/x_29.pkl', 'dataset/x_11.pkl', 'dataset/x_23.pkl', 'dataset/x_18.pkl', 'dataset/x_50.pkl', 'dataset/x_24.pkl', 'dataset/x_45.pkl', 'dataset/x_10.pkl', 'dataset/x_7.pkl', 'dataset/x_2.pkl', 'dataset/x_33.pkl', 'dataset/x_26.pkl', 'dataset/x_19.pkl', 'dataset/x_40.pkl', 'dataset/x_13.pkl', 'dataset/x_27.pkl', 'dataset/x_43.pkl', 'dataset/x_30.pkl', 'dataset/x_3.pkl', 'dataset/x_21.pkl', 'dataset/x_41.pkl', 'dataset/x_8.pkl', 'dataset/x_49.pkl', 'dataset/x_51.pkl', 'dataset/x_14.pkl', 'dataset/x_28.pkl', 'dataset/x_6.pkl', 'dataset/x_31.pkl', 'dataset/x_35.pkl', 'dataset/x_9.pkl', 'dataset/x_48.pkl', 'dataset/x_0.pkl', 'dataset/x_1.pkl', 'dataset/x_44.pkl', 'dataset/x_32.pkl', 'dataset/x_4.pkl', 'dataset/x_15.pkl', 'dataset/x_52.pkl', 'dataset/x_38.pkl', 'dataset/x_16.pkl', 'dataset/x_12.pkl', 'dataset/x_37.pkl', 'dataset/x_17.pkl', 'dataset/x_36.pkl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Pytorch Dataset"
      ],
      "metadata": {
        "id": "-n72yBqa8-vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class CustomMultiPickleDataset(Dataset):\n",
        "    def __init__(self, pickle_files, train=True, testsize=0.1):\n",
        "        \"\"\"\n",
        "        params:\n",
        "          pickle_files: List containing all pickle filepath on which dataset need to shuffle on\n",
        "          train: Whether dataset is pointing to train or test dataset\n",
        "          testsize: test dataset size\n",
        "        \"\"\"\n",
        "        self.pickle_files = pickle_files\n",
        "        self.train = train\n",
        "        self.testsize = testsize\n",
        "\n",
        "        print(f\"Total pickle files : {len(self.pickle_files)}\")\n",
        "\n",
        "        # Map containing virtual Dataset index to actual Location in pickle file.    \n",
        "        # Eg trainidx_map = {1: [2, 4]}  # meaning 1st index in training dataset is present in 4th index of 2nd pickle file\n",
        "\n",
        "        def _generate_train_indexes():\n",
        "            self.map_trainidx = {}\n",
        "            train_cntr = 0\n",
        "            for i, pickle_file in enumerate(self.pickle_files):\n",
        "                pk = pickle.load(open(pickle_file, 'rb'))\n",
        "\n",
        "                # Generating IDX map for train \n",
        "                n_samples_train = int(pk.shape[0] * (1 - testsize))            \n",
        "                for k in range(train_cntr, train_cntr + n_samples_train):\n",
        "                    self.map_trainidx[k] = [i, k - train_cntr]  # pickle file index and location in that pickle file\n",
        "                train_cntr = train_cntr + n_samples_train\n",
        "            self.total_samples_train = len(self.map_trainidx.keys())\n",
        "            print(f\"Total train - samples {self.total_samples_train}\")\n",
        "\n",
        "        def _generate_test_indexes():\n",
        "            self.map_testidx = {}\n",
        "\n",
        "            test_cntr = 0\n",
        "            for i, pickle_file in enumerate(self.pickle_files):\n",
        "                pk = pickle.load(open(pickle_file, 'rb'))\n",
        "\n",
        "                # Generating IDX map for test\n",
        "                n_samples_train = int(pk.shape[0] * (1 - testsize))            \n",
        "                n_samples_test = pk.shape[0] - n_samples_train        \n",
        "                for k in range(test_cntr, test_cntr + n_samples_test):\n",
        "                    self.map_testidx[k] = [i,  k - test_cntr + n_samples_test]\n",
        "                test_cntr = test_cntr + n_samples_test\n",
        "\n",
        "            self.total_samples_test = len(self.map_testidx.keys())\n",
        "            print(f\"Total validation - samples {self.total_samples_test}\")\n",
        "\n",
        "        if train:\n",
        "            _generate_train_indexes()\n",
        "            return\n",
        "        _generate_test_indexes()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return self.total_samples_train\n",
        "        return self.total_samples_test\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        idx_map = self.map_trainidx if self.train else self.map_testidx\n",
        "        \n",
        "        # For each ID, we need to find corresonding X and Y pickle file\n",
        "        x_filepath = self.pickle_files[idx_map[idx][0]]\n",
        "        y_filepath = x_filepath.replace('x_', 'y_')\n",
        "        rel_idx = idx_map[idx][1]\n",
        "\n",
        "        # Load the pickle file and return the sample\n",
        "        x = pickle.load(open(x_filepath, 'rb'))[rel_idx]\n",
        "        y = pickle.load(open(y_filepath, 'rb'))[rel_idx]\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "-5wyqGeWu8Up"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining Params"
      ],
      "metadata": {
        "id": "6maGCo8A9JxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "n_parallel = 4\n",
        "testsize = 0.2"
      ],
      "metadata": {
        "id": "f7fAg4Lb33bL"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading training dataset"
      ],
      "metadata": {
        "id": "szHYIqFl4WG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_train = DataLoader(CustomMultiPickleDataset(picklefiles, train=True, testsize=testsize), \n",
        "                          batch_size=batch_size, \n",
        "                          num_workers=n_parallel, \n",
        "                          shuffle=True, \n",
        "                          pin_memory=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bTXayZh3pxG",
        "outputId": "985039b5-5192-46d8-dda2-78ba663d1271"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pickle files : 53\n",
            "Total train - samples 84800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Epochs = 4\n",
        "for i in range(Epochs):\n",
        "  for j, data in enumerate(dataloader_train):\n",
        "      x, y = data\n",
        "      print(j, x.shape, y.shape)\n",
        "      # Train some model\n",
        "      break\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGxuDgur32B2",
        "outputId": "8c4cc2e3-027f-4530-8f9d-b00d6bcda268"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([8, 11, 6]) torch.Size([8, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading test dataset"
      ],
      "metadata": {
        "id": "BdwhcUKw6nvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_test = DataLoader(CustomMultiPickleDataset(picklefiles, train=False, testsize=testsize), \n",
        "                          batch_size=batch_size, \n",
        "                          num_workers=n_parallel, \n",
        "                          shuffle=True, \n",
        "                          pin_memory=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgBmsEkq6qdE",
        "outputId": "e4acf549-9eb1-48fb-bc83-f08eff97c743"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pickle files : 53\n",
            "Total validation - samples 21200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j, data in enumerate(dataloader_test):\n",
        "    x, y = data\n",
        "    print(j, x.shape, y.shape)\n",
        "    # Infer the result for testing\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afl_-hYt6sK0",
        "outputId": "8456311b-e3fa-49f7-df9c-587036cd8718"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([8, 11, 6]) torch.Size([8, 3, 6])\n"
          ]
        }
      ]
    }
  ]
}